{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14a85e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded FLPyfhelin\n"
     ]
    }
   ],
   "source": [
    "# Reload FLPyfhelin to pick up recent fixes\n",
    "import importlib\n",
    "import FLPyfhelin as fl\n",
    "importlib.reload(fl)\n",
    "from FLPyfhelin import *\n",
    "print('Reloaded FLPyfhelin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0529fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.17M/4.17M [00:01<00:00, 3.56MB/s]\n",
      "/var/folders/m7/cxy5x_m16yg7yj7qmrxswlqr0000gn/T/ipykernel_97888/1013813365.py:63: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  im = Image.fromarray(arr.astype(np.uint8), mode='L')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export complete:\n",
      " Train images: 5232\n",
      " Test images: 624\n",
      " Classes: ['0', '1']\n"
     ]
    }
   ],
   "source": [
    "# Install medmnist if needed and export dataset to folders used by this notebook\n",
    "import sys, subprocess\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Ensure medmnist is installed\n",
    "try:\n",
    "    import medmnist  # noqa: F401\n",
    "except Exception:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"medmnist>=3.0.0\"]) \n",
    "    import medmnist\n",
    "\n",
    "from medmnist import INFO\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Choose dataset\n",
    "DATA_FLAG = 'pneumoniamnist'  # binary (0/1)\n",
    "IMG_SIZE = 28\n",
    "AS_RGB = False\n",
    "\n",
    "# Output dirs expected by the rest of the notebook\n",
    "TRAIN_DIR = Path('image/Train')\n",
    "TEST_DIR  = Path('image/Test')\n",
    "for p in [TRAIN_DIR, TEST_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Map label to class folder names (keep numeric names to be safe with existing code)\n",
    "label_to_name = None  # will use numeric labels '0', '1', ...\n",
    "\n",
    "# Helper to convert tensor/array to HWC numpy\n",
    "def to_hwc(arr):\n",
    "    x = np.array(arr)\n",
    "    if x.ndim == 3 and x.shape[0] in (1,3):\n",
    "        x = np.transpose(x, (1,2,0))\n",
    "    return x\n",
    "\n",
    "# Download and write images\n",
    "info = INFO[DATA_FLAG]\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "train_ds = DataClass(split='train', download=True, size=IMG_SIZE, as_rgb=AS_RGB)\n",
    "val_ds   = DataClass(split='val',   download=True, size=IMG_SIZE, as_rgb=AS_RGB)\n",
    "test_ds  = DataClass(split='test',  download=True, size=IMG_SIZE, as_rgb=AS_RGB)\n",
    "\n",
    "# Combine train+val into Train folder to match your generators\n",
    "from itertools import chain\n",
    "splits = [(chain(range(len(train_ds)), range(len(val_ds))), train_ds, val_ds, TRAIN_DIR),\n",
    "          (range(len(test_ds)), test_ds, None, TEST_DIR)]\n",
    "\n",
    "counts = {\"train\":0, \"test\":0}\n",
    "\n",
    "def save_item(img, lab, out_root, idx):\n",
    "    # label is array-like; grab first element\n",
    "    try:\n",
    "        y = int(lab[0])\n",
    "    except Exception:\n",
    "        y = int(lab)\n",
    "    cls_dir = out_root / str(y)\n",
    "    cls_dir.mkdir(parents=True, exist_ok=True)\n",
    "    arr = to_hwc(img)\n",
    "    if arr.ndim == 2:\n",
    "        im = Image.fromarray(arr.astype(np.uint8), mode='L')\n",
    "    else:\n",
    "        im = Image.fromarray(arr.astype(np.uint8))\n",
    "    im.save(cls_dir / f\"{idx}.png\")\n",
    "\n",
    "# Write training (train+val)\n",
    "idx = 0\n",
    "for i in range(len(train_ds)):\n",
    "    img, lab = train_ds[i]\n",
    "    save_item(img, lab, TRAIN_DIR, idx)\n",
    "    idx += 1\n",
    "for i in range(len(val_ds)):\n",
    "    img, lab = val_ds[i]\n",
    "    save_item(img, lab, TRAIN_DIR, idx)\n",
    "    idx += 1\n",
    "counts[\"train\"] = idx\n",
    "\n",
    "# Write test\n",
    "idx = 0\n",
    "for i in range(len(test_ds)):\n",
    "    img, lab = test_ds[i]\n",
    "    save_item(img, lab, TEST_DIR, idx)\n",
    "    idx += 1\n",
    "counts[\"test\"] = idx\n",
    "\n",
    "print(\"Export complete:\")\n",
    "print(\" Train images:\", counts[\"train\"]) \n",
    "print(\" Test images:\", counts[\"test\"]) \n",
    "print(\" Classes:\", sorted({str(int(train_ds[i][1][0])) for i in range(min(100, len(train_ds))) }))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e24509",
   "metadata": {},
   "source": [
    "# Use MedMNIST as the dataset source\n",
    "\n",
    "This section downloads a selected MedMNIST dataset and exports images to the expected folder structure:\n",
    "- `image/Train/<class>/...png`\n",
    "- `image/Test/<class>/...png`\n",
    "\n",
    "Default is PNEUMONIAMNIST (binary classification). You can switch datasets in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ef0f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np              \n",
    "import pandas as pd          \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score,\\\n",
    "    f1_score, accuracy_score, classification_report\n",
    "#import keras_tuner as kt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from FLPyfhelin import *  # This now includes the Pyfhel shim\n",
    "import time\n",
    "\n",
    "train_path = 'image/Train'\n",
    "test_path = 'image/Test'\n",
    "#folder = 'image'\n",
    "batch_size = 32\n",
    "#num_client =2\n",
    "SCALE=1\n",
    "epoch=1\n",
    "input_size  = (int(256*SCALE), int(256*SCALE), 3)\n",
    "image_size  = (int(256*SCALE), int(256*SCALE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60a754db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FLPyfhelin.Pyfhel at 0x1611ef770>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate and export public key, just need to run it once\n",
    "HE=gen_pk(s=128, m=1024)\n",
    "keys ={}\n",
    "keys['HE'] = HE\n",
    "keys['con'] = HE.to_bytes_context()\n",
    "keys['pk'] = HE.to_bytes_publicKey()\n",
    "keys['sk'] = HE.to_bytes_secretKey()\n",
    "    \n",
    "filename =  \"privatekey.pickle\"\n",
    "with open(filename, 'wb') as handle:\n",
    "    pickle.dump(keys, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "HE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a707604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FLPyfhelin.Pyfhel at 0x177fb6f90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename =  \"privatekey.pickle\"\n",
    "with open(filename, 'rb') as handle:\n",
    "        key = pickle.load(handle)\n",
    "\n",
    "HE = key['HE']\n",
    "HE.from_bytes_context(key['con'])\n",
    "HE.from_bytes_publicKey(key['pk'])\n",
    "HE.from_bytes_secretKey(key['sk'])\n",
    "HE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de270715",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clients:  2\n",
      "Prepare dataset\n",
      "Found 624 validated image filenames belonging to 2 classes.\n",
      "Build main model\n",
      "Build main model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/CODES/FL with HE/.venv/lib/python3.13/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/Applications/CODES/FL with HE/.venv/lib/python3.13/site-packages/keras/src/optimizers/base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Clients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2355 validated image filenames belonging to 2 classes.\n",
      "Found 261 validated image filenames belonging to 2 classes.\n",
      "Found 261 validated image filenames belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/CODES/FL with HE/.venv/lib/python3.13/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721ms/step - accuracy: 0.7006 - loss: 0.6003\n",
      "Epoch 1: accuracy improved from None to 0.73546, saving model to weights/client_1.weights.h5\n",
      "\n",
      "Epoch 1: accuracy improved from None to 0.73546, saving model to weights/client_1.weights.h5\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 752ms/step - accuracy: 0.7355 - loss: 0.5838 - val_accuracy: 0.7203 - val_loss: 0.6153 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 752ms/step - accuracy: 0.7355 - loss: 0.5838 - val_accuracy: 0.7203 - val_loss: 0.6153 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 761ms/step - accuracy: 0.7569 - loss: 0.5397\n",
      "Epoch 2: accuracy improved from 0.73546 to 0.78471, saving model to weights/client_1.weights.h5\n",
      "\n",
      "Epoch 2: accuracy improved from 0.73546 to 0.78471, saving model to weights/client_1.weights.h5\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 787ms/step - accuracy: 0.7847 - loss: 0.4685 - val_accuracy: 0.8812 - val_loss: 0.2538 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 787ms/step - accuracy: 0.7847 - loss: 0.4685 - val_accuracy: 0.8812 - val_loss: 0.2538 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640ms/step - accuracy: 0.8479 - loss: 0.3583\n",
      "Epoch 3: accuracy improved from 0.78471 to 0.86752, saving model to weights/client_1.weights.h5\n",
      "\n",
      "Epoch 3: accuracy improved from 0.78471 to 0.86752, saving model to weights/client_1.weights.h5\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 671ms/step - accuracy: 0.8675 - loss: 0.3221 - val_accuracy: 0.8582 - val_loss: 0.3049 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 671ms/step - accuracy: 0.8675 - loss: 0.3221 - val_accuracy: 0.8582 - val_loss: 0.3049 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704ms/step - accuracy: 0.8778 - loss: 0.2747\n",
      "Epoch 4: accuracy improved from 0.86752 to 0.88195, saving model to weights/client_1.weights.h5\n",
      "\n",
      "Epoch 4: accuracy improved from 0.86752 to 0.88195, saving model to weights/client_1.weights.h5\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 731ms/step - accuracy: 0.8820 - loss: 0.2792 - val_accuracy: 0.9195 - val_loss: 0.2216 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 731ms/step - accuracy: 0.8820 - loss: 0.2792 - val_accuracy: 0.9195 - val_loss: 0.2216 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700ms/step - accuracy: 0.8996 - loss: 0.2505\n",
      "Epoch 5: accuracy improved from 0.88195 to 0.89724, saving model to weights/client_1.weights.h5\n",
      "\n",
      "Epoch 5: accuracy improved from 0.88195 to 0.89724, saving model to weights/client_1.weights.h5\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 730ms/step - accuracy: 0.8972 - loss: 0.2430 - val_accuracy: 0.9310 - val_loss: 0.1845 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 730ms/step - accuracy: 0.8972 - loss: 0.2430 - val_accuracy: 0.9310 - val_loss: 0.1845 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788ms/step - accuracy: 0.8905 - loss: 0.2442\n",
      "Epoch 6: accuracy improved from 0.89724 to 0.89809, saving model to weights/client_1.weights.h5\n",
      "\n",
      "Epoch 6: accuracy improved from 0.89724 to 0.89809, saving model to weights/client_1.weights.h5\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 818ms/step - accuracy: 0.8981 - loss: 0.2395 - val_accuracy: 0.9119 - val_loss: 0.2111 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 818ms/step - accuracy: 0.8981 - loss: 0.2395 - val_accuracy: 0.9119 - val_loss: 0.2111 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742ms/step - accuracy: 0.9105 - loss: 0.2206\n",
      "Epoch 7: accuracy improved from 0.89809 to 0.90361, saving model to weights/client_1.weights.h5\n",
      "\n",
      "Epoch 7: accuracy improved from 0.89809 to 0.90361, saving model to weights/client_1.weights.h5\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 770ms/step - accuracy: 0.9036 - loss: 0.2294 - val_accuracy: 0.9387 - val_loss: 0.1722 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 770ms/step - accuracy: 0.9036 - loss: 0.2294 - val_accuracy: 0.9387 - val_loss: 0.1722 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686ms/step - accuracy: 0.9089 - loss: 0.2267\n",
      "Epoch 8: accuracy improved from 0.90361 to 0.90786, saving model to weights/client_1.weights.h5\n",
      "\n",
      "Epoch 8: accuracy improved from 0.90361 to 0.90786, saving model to weights/client_1.weights.h5\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 712ms/step - accuracy: 0.9079 - loss: 0.2267 - val_accuracy: 0.9464 - val_loss: 0.1697 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 712ms/step - accuracy: 0.9079 - loss: 0.2267 - val_accuracy: 0.9464 - val_loss: 0.1697 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703ms/step - accuracy: 0.9226 - loss: 0.2021\n",
      "Epoch 9: accuracy improved from 0.90786 to 0.92484, saving model to weights/client_1.weights.h5\n",
      "\n",
      "Epoch 9: accuracy improved from 0.90786 to 0.92484, saving model to weights/client_1.weights.h5\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 730ms/step - accuracy: 0.9248 - loss: 0.1966 - val_accuracy: 0.9349 - val_loss: 0.1965 - learning_rate: 0.0010\n",
      "Epoch 10/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 730ms/step - accuracy: 0.9248 - loss: 0.1966 - val_accuracy: 0.9349 - val_loss: 0.1965 - learning_rate: 0.0010\n",
      "Epoch 10/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696ms/step - accuracy: 0.9141 - loss: 0.2045\n",
      "Epoch 10: accuracy did not improve from 0.92484\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 724ms/step - accuracy: 0.9189 - loss: 0.1972 - val_accuracy: 0.9234 - val_loss: 0.1921 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 10: accuracy did not improve from 0.92484\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 724ms/step - accuracy: 0.9189 - loss: 0.1972 - val_accuracy: 0.9234 - val_loss: 0.1921 - learning_rate: 0.0010\n",
      "Found 2355 validated image filenames belonging to 2 classes.\n",
      "Found 261 validated image filenames belonging to 2 classes.\n",
      "Found 2355 validated image filenames belonging to 2 classes.\n",
      "Found 261 validated image filenames belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/CODES/FL with HE/.venv/lib/python3.13/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699ms/step - accuracy: 0.9284 - loss: 0.2081\n",
      "Epoch 1: accuracy improved from None to 0.92654, saving model to weights/client_2.weights.h5\n",
      "\n",
      "Epoch 1: accuracy improved from None to 0.92654, saving model to weights/client_2.weights.h5\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 731ms/step - accuracy: 0.9265 - loss: 0.1921 - val_accuracy: 0.9080 - val_loss: 0.2326 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 731ms/step - accuracy: 0.9265 - loss: 0.1921 - val_accuracy: 0.9080 - val_loss: 0.2326 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656ms/step - accuracy: 0.9349 - loss: 0.1816\n",
      "Epoch 2: accuracy improved from 0.92654 to 0.93376, saving model to weights/client_2.weights.h5\n",
      "\n",
      "Epoch 2: accuracy improved from 0.92654 to 0.93376, saving model to weights/client_2.weights.h5\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 683ms/step - accuracy: 0.9338 - loss: 0.1790 - val_accuracy: 0.9272 - val_loss: 0.2019 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 683ms/step - accuracy: 0.9338 - loss: 0.1790 - val_accuracy: 0.9272 - val_loss: 0.2019 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703ms/step - accuracy: 0.9213 - loss: 0.2013\n",
      "Epoch 3: accuracy did not improve from 0.93376\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 730ms/step - accuracy: 0.9244 - loss: 0.1961 - val_accuracy: 0.9119 - val_loss: 0.2129 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 3: accuracy did not improve from 0.93376\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 730ms/step - accuracy: 0.9244 - loss: 0.1961 - val_accuracy: 0.9119 - val_loss: 0.2129 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665ms/step - accuracy: 0.9319 - loss: 0.1690\n",
      "Epoch 4: accuracy did not improve from 0.93376\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 693ms/step - accuracy: 0.9223 - loss: 0.1940 - val_accuracy: 0.9004 - val_loss: 0.2048 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 4: accuracy did not improve from 0.93376\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 693ms/step - accuracy: 0.9223 - loss: 0.1940 - val_accuracy: 0.9004 - val_loss: 0.2048 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669ms/step - accuracy: 0.9333 - loss: 0.1894\n",
      "Epoch 5: accuracy did not improve from 0.93376\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 695ms/step - accuracy: 0.9325 - loss: 0.1851 - val_accuracy: 0.9272 - val_loss: 0.1776 - learning_rate: 3.0000e-04\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 5: accuracy did not improve from 0.93376\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 695ms/step - accuracy: 0.9325 - loss: 0.1851 - val_accuracy: 0.9272 - val_loss: 0.1776 - learning_rate: 3.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678ms/step - accuracy: 0.9348 - loss: 0.1693\n",
      "Epoch 6: accuracy improved from 0.93376 to 0.93503, saving model to weights/client_2.weights.h5\n",
      "\n",
      "Epoch 6: accuracy improved from 0.93376 to 0.93503, saving model to weights/client_2.weights.h5\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 706ms/step - accuracy: 0.9350 - loss: 0.1665 - val_accuracy: 0.9349 - val_loss: 0.1895 - learning_rate: 3.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 706ms/step - accuracy: 0.9350 - loss: 0.1665 - val_accuracy: 0.9349 - val_loss: 0.1895 - learning_rate: 3.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507ms/step - accuracy: 0.9350 - loss: 0.1654\n",
      "Epoch 7: accuracy improved from 0.93503 to 0.93716, saving model to weights/client_2.weights.h5\n",
      "\n",
      "Epoch 7: accuracy improved from 0.93503 to 0.93716, saving model to weights/client_2.weights.h5\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 530ms/step - accuracy: 0.9372 - loss: 0.1595 - val_accuracy: 0.9349 - val_loss: 0.1656 - learning_rate: 3.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 530ms/step - accuracy: 0.9372 - loss: 0.1595 - val_accuracy: 0.9349 - val_loss: 0.1656 - learning_rate: 3.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538ms/step - accuracy: 0.9475 - loss: 0.1490\n",
      "Epoch 8: accuracy improved from 0.93716 to 0.94352, saving model to weights/client_2.weights.h5\n",
      "\n",
      "Epoch 8: accuracy improved from 0.93716 to 0.94352, saving model to weights/client_2.weights.h5\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 562ms/step - accuracy: 0.9435 - loss: 0.1508 - val_accuracy: 0.9502 - val_loss: 0.1730 - learning_rate: 3.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 562ms/step - accuracy: 0.9435 - loss: 0.1508 - val_accuracy: 0.9502 - val_loss: 0.1730 - learning_rate: 3.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539ms/step - accuracy: 0.9446 - loss: 0.1554\n",
      "Epoch 9: accuracy did not improve from 0.94352\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 562ms/step - accuracy: 0.9397 - loss: 0.1585 - val_accuracy: 0.9387 - val_loss: 0.1861 - learning_rate: 3.0000e-04\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 9: accuracy did not improve from 0.94352\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 562ms/step - accuracy: 0.9397 - loss: 0.1585 - val_accuracy: 0.9387 - val_loss: 0.1861 - learning_rate: 3.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545ms/step - accuracy: 0.9379 - loss: 0.1677\n",
      "Epoch 10: accuracy improved from 0.94352 to 0.94437, saving model to weights/client_2.weights.h5\n",
      "\n",
      "Epoch 10: accuracy improved from 0.94352 to 0.94437, saving model to weights/client_2.weights.h5\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 568ms/step - accuracy: 0.9444 - loss: 0.1523 - val_accuracy: 0.9540 - val_loss: 0.1698 - learning_rate: 3.0000e-04\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 568ms/step - accuracy: 0.9444 - loss: 0.1523 - val_accuracy: 0.9540 - val_loss: 0.1698 - learning_rate: 3.0000e-04\n",
      "Export client weights\n",
      "Export client weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/CODES/FL with HE/.venv/lib/python3.13/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/Applications/CODES/FL with HE/.venv/lib/python3.13/site-packages/keras/src/optimizers/base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to encrypt weights: 0.6247076988220215\n",
      "Time to export weights to pickle: 0.14200711250305176\n",
      "Weights exported: Client 1\n",
      "Time to encrypt weights: 0.19739508628845215\n",
      "Time to export weights to pickle: 0.11830019950866699\n",
      "Weights exported: Client 2\n",
      "Total time to encrypt and export: 1.196160078048706\n",
      "Aggregate Weights\n",
      "Time to encrypt weights: 0.19739508628845215\n",
      "Time to export weights to pickle: 0.11830019950866699\n",
      "Weights exported: Client 2\n",
      "Total time to encrypt and export: 1.196160078048706\n",
      "Aggregate Weights\n",
      "Time to import: 0.09758400917053223\n",
      "Time to import: 0.09758400917053223\n",
      "Time to import: 0.07823896408081055\n",
      "Time to import: 0.07823896408081055\n",
      "Time to aggregate: 0.8073930740356445\n",
      "Export Aggregate Weights\n",
      "Time to export weights to pickle: 0.11856627464294434\n",
      "Time to aggregate: 0.8073930740356445\n",
      "Export Aggregate Weights\n",
      "Time to export weights to pickle: 0.11856627464294434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to import: 0.0783848762512207\n",
      "Time to decrypt: 0.14589905738830566\n",
      "Run predictions on aggregated model\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 155ms/step\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 155ms/step\n",
      "Run prepare confusion matrix\n",
      "Run prepare confusion matrix\n"
     ]
    }
   ],
   "source": [
    "num_of_client_list = [2]\n",
    "prec_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "acc_list = []\n",
    "t =[]\n",
    "\n",
    "for j, num_client in enumerate(num_of_client_list):\n",
    "        print('Number of clients: ',num_client)\n",
    "        start = time.time()\n",
    "\n",
    "         \n",
    "        print('Prepare dataset')\n",
    "        df_train = prep_df(train_path,shuffle=True)\n",
    "        df_test = prep_df(test_path,shuffle=False)\n",
    "        test_ds = get_test_data(df_test,train_path)\n",
    "        \n",
    "        print('Build main model')\n",
    "        model = create_model()\n",
    "        model.save('main_model.hdf5')\n",
    "\n",
    "        print(\"Train Clients\")\n",
    "        train_clients(df_train, train_path, num_client,epoch)\n",
    "                \n",
    "        print(\"Export client weights\")\n",
    "        export_encrypted_clients_weights(num_client)\n",
    "                \n",
    "        print(\"Aggregate Weights\")\n",
    "        main_model_dict = aggregate_encrypted_weights(num_client)\n",
    "        filename=\"weights/aggregated.pickle\"\n",
    "        print(\"Export Aggregate Weights\")\n",
    "        export_weights(filename ,main_model_dict)\n",
    "                \n",
    "        agg_model = decrypt_import_weights(filename)\n",
    "\n",
    "        print(\"Run predictions on aggregated model\")\n",
    "        preds = agg_model.predict(test_ds,verbose=1)\n",
    "        predictions = preds.copy()\n",
    "        predictions = [np.argmax(x) for x in predictions]\n",
    "        \n",
    "        print(\"Run prepare confusion matrix\")\n",
    "        prec_list.append(precision_score(test_ds.classes, predictions,average='weighted'))\n",
    "        recall_list.append(recall_score(test_ds.classes, predictions,average='weighted'))\n",
    "        f1_list.append(f1_score(test_ds.classes, predictions,average='weighted'))\n",
    "        acc_list.append(accuracy_score(test_ds.classes, predictions))\n",
    "        end = time.time()\n",
    "        t.append(end-start)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46616942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.888952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.889423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.888636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.889423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  2\n",
       "Precision  0.888952\n",
       "Recall     0.889423\n",
       "F1 Score   0.888636\n",
       "Accuracy   0.889423"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_list =[prec_list,recall_list,f1_list,acc_list]\n",
    "client = num_of_client_list\n",
    "idx=['Precision','Recall','F1 Score', 'Accuracy']\n",
    "\n",
    "df = pd.DataFrame(sum_list, index=idx, columns=client)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c175d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <td>1037.087127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                2\n",
       "Time  1037.087127"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_list =[t]\n",
    "client = num_of_client_list\n",
    "idx=['Time']\n",
    "\n",
    "df = pd.DataFrame(time_list, index=idx, columns=client)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed31f6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to export weights to pickle: 0.0012409687042236328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/CODES/FL with HE/.venv/lib/python3.13/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/Applications/CODES/FL with HE/.venv/lib/python3.13/site-packages/keras/src/optimizers/base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model=load_weights('1')\n",
    "encrypted_weights={}\n",
    "\n",
    "for i in range(len(model.layers)):\n",
    "        if model.layers[i].get_weights()!=[]:\n",
    "            encrypted =[]\n",
    "            weights = model.layers[i].get_weights()   \n",
    "\n",
    "            for j in range(len(weights)):\n",
    "                    array= weights[j]\n",
    "                    encrypted_weights['c_'+str(i)+'_'+str(j)]=array\n",
    "\n",
    "filename =  \"plainweights.pickle\"\n",
    "export_weights(filename, encrypted_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
